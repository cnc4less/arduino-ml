{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes, Part 1\n",
    "\n",
    "Derived from:\n",
    "\n",
    "https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/\n",
    "https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf\n",
    "\n",
    "### What is Naive Bayes?\n",
    "Naive Bayes is a probabilistic machine learning algorithm. It is based on Bayes' theorem. This algorithm makes an assumption that all features are independent of each other. In other words, changing the value of one feature, does not directly change the value of any of the other features. This assumption is naive because it is (almost) never true.\n",
    "\n",
    "For example, if you have temperature and humidity as input features, Naive Bayes assumes that temperature and humidity are independent of each other. So, changing the value of temperature, does not directly change the value of humidity. However, in reality, temperature and humidity are related to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability\n",
    "Before you go into Naive Bayes, you need to understand what Conditional Probability is. Let's start with an example.\n",
    "\n",
    "When you toss a fair coin, it has a probability of 1/2 of getting heads or tails. Mathematically, it is written as\n",
    "$P(Head)=\\frac{1}{2}$ and $P(Tail)=\\frac{1}{2}$.\n",
    "\n",
    "Another example, suppose you pick a card from the deck and you already know that your card is an ace. What is the probability of getting a diamond given the card is an ace? Well, we have already a condition that the card is an ace. So, the population (denominator) is 4 not 52. There is only one diamond in aces. So, the probability of getting a diamond given the card is an ace is 1/4. Mathematically, it is written as $P(Diamond|Ace)=\\frac{1}{4}$. This is called conditional probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes by Example 1\n",
    "Let's start with a simple example of Naive Bayes algorithm. Suppose you have 100 fruits which could be either apple or orange. Your training data of these fruits is shown in the following table. In this training data, we have one feature which is color that has three possible values: red, green, and orange. Then, we have one label that has two possible values: apple and orange. We denote color as $X$ and fruit as $y$.\n",
    "\n",
    "| No. | Color  | Fruit  |\n",
    "|-----|--------|--------|\n",
    "| 1   | Red    | Apple  |\n",
    "| 2   | Red    | Apple  |\n",
    "| 3   | Green  | Apple  |\n",
    "| 4   | Green  | Orange |\n",
    "| 5   | Orange | Orange |\n",
    "| ... | ...    | ...    |\n",
    "| 100 | Green  | Orange |\n",
    "\n",
    "The objective of Naive Bayes classifier is to predict if a given fruit (by knowing its color) is an apple or orange. Let's say the color of a fruit is green ($X=green$), can you predict what fruit ($y$) is it? In other words, you can predict $y$ when only the $X$ variables in training data are known.\n",
    "\n",
    "The idea is to compute the two probabilities, that is the probability of the fruit being an apple or orange. Whichever fruit type gets the highest probability wins. Mathematically, you need to compute both $P(y=apple|X=green)$ and $P(y=orange|X=green)$, then compare the results. If $P(y=apple|X=green) > P(y=orange|X=green)$ then the prediction is an apple. If $P(y=apple|X=green) < P(y=orange|X=green)$ then the prediction is an orange.\n",
    "\n",
    "This is the Naive Bayes formula for computing these probabilities:\n",
    "$$P(y=apple|X=green) = \\frac{P(X=green|y=apple)P(y=apple)}{P(X=green)}$$\n",
    "$$P(y=orange|X=green) = \\frac{P(X=green|y=orange)P(y=orange)}{P(X=green)}$$\n",
    "\n",
    "The step-by-step of Naive Bayes algorithm:\n",
    "\n",
    "#### **Step 0: create a frequency table for each feature of the training data**\n",
    "First, you need to build a table called frequency table for each feature of the training data. Since we have only one feature, which is color, so we only need to build one frequency table.\n",
    "\n",
    "Given the training data, you need to count how many red apples, green apples and orange apples are there. The same for the oranges, you need to count how many red oranges, green oranges and orange oranges are there.\n",
    "\n",
    "| No. | Color  | Fruit  |\n",
    "|-----|--------|--------|\n",
    "| 1   | Red    | Apple  |\n",
    "| 2   | Red    | Apple  |\n",
    "| 3   | Green  | Apple  |\n",
    "| 4   | Green  | Orange |\n",
    "| 5   | Orange | Orange |\n",
    "| ... | ...    | ...    |\n",
    "| 100 | Green  | Orange |\n",
    "\n",
    "I have omitted the other rows for the sake of simplicity. The following table shows the frequency table. We have 33 red apples, 0 red orange, 20 green apples, 7 green oranges, 1 orange apple, and 39 orange oranges.\n",
    "\n",
    "| Color  | Apple | Orange | Total |\n",
    "|--------|-------|--------|-------|\n",
    "| Red    | 33    | 0      | 33    |\n",
    "| Green  | 20    | 7      | 27    |\n",
    "| Orange | 1     | 39     | 40    |\n",
    "| Total  | 54    | 46     | 100   |\n",
    "\n",
    "Finally, you need to add them together to get the total number of apples, oranges, and the total number of fruits are there. The same for every color, you need to add them together to get the total number red fruits, green fruits, orange fruits, and the total number of fruits are there.\n",
    "\n",
    "#### **Step 1: compute the probabilities for each of the fruits (label)**\n",
    "In this step, you need to compute the $P(y=apple)$ and $P(y=orange)$. Out of 100 fruits, you have 54 apples and 46 oranges. So the respective probabilities are:\n",
    "$$P(y=apple)=\\frac{54}{100}$$\n",
    "$$P(y=orange)=\\frac{46}{100}$$\n",
    "\n",
    "#### **Step 2: compute the probability of the color (feature)**\n",
    "In this step, you need to compute $P(X=green)$. Out of 100 fruits, you have 27 greens. So the probability is:\n",
    "$$P(X=green)=\\frac{27}{100}$$\n",
    "\n",
    "#### **Step 3: compute the conditional probabilities**\n",
    "In this step, you need to compute $P(X=green|y=apple)$ and $P(X=green|y=orange)$. Out of 54 apples, you have 20 greens. So the probability is:\n",
    "$$P(X=green|y=apple)=\\frac{20}{54}$$\n",
    "Out of 46 oranges, you have 7 greens. So the probability is:\n",
    "$$P(X=green|y=orange)=\\frac{7}{46}$$\n",
    "\n",
    "#### **Step 4: substitute all the three probabilities into the Naive Bayes formula**\n",
    "In this step, you need to subtitute all the three probabilities into the Naive Bayes formula.\n",
    "$$P(y=apple|X=green) = \\frac{\\frac{20}{54}\\frac{54}{100}}{\\frac{27}{100}}=\\frac{20}{27}$$\n",
    "$$P(y=orange|X=green) = \\frac{\\frac{7}{46}\\frac{46}{100}}{\\frac{27}{100}}=\\frac{7}{27}$$\n",
    "\n",
    "$P(y=apple|X=green)>P(y=orange|X=green)$, which means apple get higher probability than orange, therefore apple will be our predicted fruit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Naive Bayes Formula\n",
    "From the previous example, you can rewrite the Naive Bayes formula in more general form:\n",
    "$$P(y|X) = \\frac{P(X|y)P(y)}{P(X)}$$\n",
    "where $X$ is input feature and $y$ is output label that you want to predict. $P(y|X)$ is called posterior probability. $P(X|y)$ is called likelihood. $P(y)$ is called label/class prior probability. $P(X)$ is called feature/predictor prior probability.\n",
    "\n",
    "Now, if you notice in step 4, in the previous example, the value of denominators of both formulas remain constant ($\\frac{27}{100}$) for given input. Since we compare these two probabilities, you can remove that term. So, for Naive Bayes **<u>classifier</u>** you can simplify the formula to:\n",
    "$$P(y|X)\\propto P(X|y)P(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace Smoothing\n",
    "In the previous example, the $P(X=red|y=orange)$ is zero. It makes sense because out of 46 oranges you have 0 red, but if you have many input features, the entire probability will become zero because one of the featureâ€™s value is zero. It will wipe out all the information in the other probabilities. This case is called zero frequency, and it needs to be avoided. You can use Laplace Smoothing to solve the problem of zero probability.\n",
    "\n",
    "Laplace Smoothing can best be explained by an example. Let's say we want to compute $P(X=red|y=orange)$.\n",
    "\n",
    "Without using Laplace Smoothing, the probability is\n",
    "$$P(X=red|y=orange)=\\frac{0}{46}=0$$\n",
    "It gives us a probability of zero.\n",
    "\n",
    "Laplace Smoothing is usullay applied by **<u>adding one count to the numerator and adding number of possible values of feature to the denominator</u>**. In this case, number of possible values of feature is three (red, green, and orange). So, the probability after Laplace Smoothing is\n",
    "$$P(X=red|y=orange)=\\frac{0+1}{46+3}=\\frac{1}{49}$$\n",
    "So, by using Laplace Smoothing, it gives us a non-zero probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though Naive Bayes is naive, it perform very well in such applications, even when the features are not independent of each other. Furthermore, compared to other algorithms, Naive Bayes is fast. So, it could be used for making predictions in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
