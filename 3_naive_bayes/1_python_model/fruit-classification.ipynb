{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes, Part 1\n",
    "\n",
    "Derived from:\n",
    "\n",
    "https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/\n",
    "https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf\n",
    "https://towardsdatascience.com/introduction-to-na%C3%AFve-bayes-classifier-fa59e3e24aaf\n",
    "\n",
    "### What is Naive Bayes?\n",
    "Naive Bayes is a probabilistic machine learning algorithm. It is based on Bayes' theorem. This algorithm makes an assumption that all features are independent of each other. In other words, changing the value of one feature, does not directly change the value of any of the other features. This assumption is naive because it is (almost) never true.\n",
    "\n",
    "For example, if you have temperature and humidity as input features, Naive Bayes assumes that temperature and humidity are independent of each other. So, changing the value of temperature, does not directly change the value of humidity. In reality, temperature and humidity are related to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability\n",
    "Before you go into Naive Bayes, you need to understand what Conditional Probability is. Let's start with an example.\n",
    "\n",
    "When you toss a fair coin, it has a probability of 1/2 of getting heads or tails. Mathematically, it is written as\n",
    "$P(Head)=\\frac{1}{2}$ and $P(Tail)=\\frac{1}{2}$.\n",
    "\n",
    "Another example, suppose you pick a card from the deck and you already know that your card is an ace. What is the probability of getting a diamond given the card is an ace? Well, we have already a condition that the card is an ace. So, the population (denominator) is 4 not 52. There is only one diamond in aces. So, the probability of getting a diamond given the card is an ace is 1/4. Mathematically, it is written as $P(Diamond|Ace)=\\frac{1}{4}$. This is called conditional probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes by Example 1\n",
    "Let's start with a simple example of Naive Bayes algorithm. Suppose you have 100 fruits which could be either apple or orange. Your training data of these fruits is shown in the following table. In this training data, we have one feature which is color that has three possible values: red, green, and orange. Then, we have one label that has two possible values: apple and orange. We denote color as $X$ and fruit as $y$.\n",
    "\n",
    "| No. | Color  | Fruit  |\n",
    "|-----|--------|--------|\n",
    "| 1   | Red    | Apple  |\n",
    "| 2   | Red    | Apple  |\n",
    "| 3   | Green  | Apple  |\n",
    "| 4   | Green  | Orange |\n",
    "| 5   | Orange | Orange |\n",
    "| ... | ...    | ...    |\n",
    "| 100 | Green  | Orange |\n",
    "\n",
    "The objective of Naive Bayes classifier is to predict if a given fruit (by knowing its color) is an apple or orange. Let's say the color of a fruit is green ($X=green$), can you predict what fruit ($y$) is it? In other words, you can predict $y$ when only the $X$ variables in training data are known.\n",
    "\n",
    "The idea is to compute the two probabilities, that is the probability of the fruit being an apple or orange. Whichever fruit type gets the highest probability wins. Mathematically, you need to compute both $P(y=apple|X=green)$ and $P(y=orange|X=green)$, then compare the results. If $P(y=apple|X=green) > P(y=orange|X=green)$ then the prediction is apple. If $P(y=apple|X=green) < P(y=orange|X=green)$ then the prediction is orange.\n",
    "\n",
    "This is the Naive Bayes formula for computing these probability:\n",
    "$$P(y=apple|X=green) = \\frac{P(X=green|y=apple)P(y=apple)}{P(X=green)}$$\n",
    "$$P(y=orange|X=green) = \\frac{P(X=green|y=orange)P(y=orange)}{P(X=green)}$$\n",
    "\n",
    "These probabilities can be calculated by using information in a table called frequency table which is derived from training data. The following table shows the frequency table.\n",
    "\n",
    "|        | Apple | Orange | Total |\n",
    "|--------|-------|--------|-------|\n",
    "| Red    | 33    | 0      | 33    |\n",
    "| Green  | 20    | 7      | 27    |\n",
    "| Orange | 1     | 39     | 40    |\n",
    "| Total  | 54    | 46     | 100   |\n",
    "\n",
    "#### **Step 1: compute the probabilities for each of the fruits (label)**\n",
    "Out of 100 fruits, you have 54 apples and 46 oranges. So the respective probabilities are:\n",
    "$$P(y=apple)=\\frac{54}{100}$$\n",
    "$$P(y=orange)=\\frac{46}{100}$$\n",
    "\n",
    "#### **Step 2: compute the probabilities of the color (feature)**\n",
    "Out of 100 fruits, you have 27 greens. So the probability is:\n",
    "$$P(X=green)=\\frac{27}{100}$$\n",
    "\n",
    "#### **Step 3: compute the conditional probability**\n",
    "Out of 54 apples, you have 20 greens. So the probability is:\n",
    "$$P(X=green|y=apple)=\\frac{20}{54}$$\n",
    "Out of 46 oranges, you have 7 greens. So the probability is:\n",
    "$$P(X=green|y=orange)=\\frac{7}{46}$$\n",
    "\n",
    "#### **Step 4: subtitute all the three probabilities into the Naive Bayes formula**\n",
    "$$P(y=apple|X=green) = \\frac{\\frac{20}{54}\\frac{54}{100}}{\\frac{27}{100}}=\\frac{20}{27}$$\n",
    "$$P(y=orange|X=green) = \\frac{\\frac{7}{46}\\frac{46}{100}}{\\frac{27}{100}}=\\frac{7}{27}$$\n",
    "\n",
    "Since $P(y=apple|X=green) > P(y=orange|X=green)$, which means apple get higher probability than orange, then apple will be our predicted fruit ($y$) given the color is green ($X=green$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Naive Bayes Formula\n",
    "From the above example, you can rewrite the Naive Bayes formula in more general form:\n",
    "$$P(y|X) = \\frac{P(X|y)P(y)}{P(X)}$$\n",
    "where $X$ is input feature and $y$ is output label that you want to predict. $P(y|X)$ is called posterior probability. $P(X|y)$ is called likelihood probability. $P(y)$ is called label prior probability. $P(X)$ is called feature prior probability.\n",
    "\n",
    "Now, if you notice in step 4 in the above example, the value of denominators of both formulas remain constant ($\\frac{27}{100}$) for given input. Therefore, you can remove that term. So, for Naive Bayes **<u>classifier</u>** you can simplify the formula to:\n",
    "$$P(y|X)\\propto P(X|y)P(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace Smoothing\n",
    "In the above example, the probability of $P(X=red|y=orange)$ is zero. It makes sense because out of 46 oranges you have 0 red, but if you have many input features, the entire probability will become zero because one of the featureâ€™s value is zero. It will wipe out all the information in the other probabilities. This case is called zero frequency, and it needs to be avoided. You can use Laplace Smoothing to solve the problem of zero probability.\n",
    "\n",
    "Laplace Smoothing can best be explained by an example. Let's say we want to calculate $P(y=orange|X=red)$. Laplace Smoothing needs to be applied to both $P(X|y)$ and $P(y)$.\n",
    "\n",
    "#### **Step 1: compute the probabilities for each of the fruit/label**\n",
    "Without Laplace Smoothing, the probability is\n",
    "$$P(y=orange)=\\frac{46}{100}$$\n",
    "Laplace Smoothing is usually done by **<u>adding one to the numerator and adding number of possible value of label to the denominator</u>**. In this case, number of possible value of label is two (apple and orange). So, the probability after Laplace Smoothing is\n",
    "$$P(y=orange)=\\frac{46+1}{100+2}=\\frac{47}{102}$$\n",
    "\n",
    "#### **Step 2: compute the conditional probability**\n",
    "Without Laplace Smoothing, the probability is\n",
    "$$P(X=red|y=orange)=\\frac{0}{46}$$\n",
    "Laplace Smoothing is also applied to the conditional probability by **<u>adding one to the numerator and adding number of possible value of feature to the denominator</u>**. In this case, number of possible value of feature is three (red, green, and orange). So, the probability after Laplace Smoothing is\n",
    "$$P(X=red|y=orange)=\\frac{0+1}{46+3}=\\frac{1}{49}$$\n",
    "So, by using Laplace Smoothing, it gives us a non-zero probability.\n",
    "\n",
    "#### **Step 3: subtitute all the two probabilities into the simplified Naive Bayes formula**\n",
    "$$P(y=orange|X=red)=\\frac{1}{49}\\frac{47}{102}=\\frac{47}{4998}\\approx 0.0094037615$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
