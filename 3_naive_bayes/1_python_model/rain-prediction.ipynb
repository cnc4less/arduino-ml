{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes, Part 2\n",
    "\n",
    "Derived from:\n",
    "\n",
    "https://stackoverflow.com/questions/48177318/what-does-this-arg-max-notation-mean-in-the-scikit-learn-docs-for-naive-bayes\n",
    "\n",
    "### Naive Bayes by Example 2\n",
    "In part 1, you have seen a simple example of Naive Bayes classifier for fruit calssification. The model only has one input feature which is color. Now let's start with a slightly complex example that has more than one input feature.\n",
    "\n",
    "Consider a fictional training data that describes the weather conditions as shown in the following table. Given the weather conditions, each tuple classifies the conditions as will it rain ('Yes') or not ('No').\n",
    "\n",
    "| No. | Outlook | Temperature | Humidity | Rain |\n",
    "|-----|---------|-------------|----------|------|\n",
    "| 1   | Cloudy  | Cool        | High     | Yes  |\n",
    "| 2   | Sunny   | Mild        | High     | No   |\n",
    "| 3   | Cloudy  | Hot         | Normal   | No   |\n",
    "| 4   | Sunny   | Cool        | Normal   | No   |\n",
    "| 5   | Sunny   | Hot         | Low      | No   |\n",
    "| 6   | Cloudy  | Mild        | Normal   | Yes  |\n",
    "| 7   | Cloudy  | Hot         | High     | Yes  |\n",
    "| 8   | Cloudy  | Cool        | Low      | No   |\n",
    "| 9   | Sunny   | Cool        | High     | Yes  |\n",
    "| 10  | Sunny   | Hot         | High     | No   |\n",
    "| 11  | Cloudy  | Hot         | Low      | No   |\n",
    "| 12  | Cloudy  | Cool        | Normal   | Yes  |\n",
    "| 13  | Sunny   | Mild        | Low      | No   |\n",
    "| 14  | Cloudy  | Cool        | Low      | Yes  |\n",
    "| 15  | Sunny   | Mild        | Low      | No   |\n",
    "\n",
    "In this training data, we have three features which are outlook, temperature, and humidity. Outlook has two possible values: cloudy and sunny. Temperature has three possible values: cool, mild, and hot. Humidity has three possible values: low, normal, and high. Then, we have one label that has two possible values: yes and no.\n",
    "\n",
    "We denote features as $X_{i}$ where $i=\\{1, 2, 3\\}$ that corresponds to outlook, temperature, and humidity, repectively. We denote label as $y$. Naive Bayes classifier assumes each of the features is independence. In fact, the independence assumption is never true, but often works well in practice.\n",
    "\n",
    "In part 1, you have Naive Bayes classifier as follow:\n",
    "$$P(y|X)\\propto P(X|y)P(y)$$\n",
    "where $X$ is input feature and $y$ is output label that you want to predict. If you have more than one feature, then the Naive Bayes classifier formula becomes:\n",
    "$$P(y|X_{1},...,X_{n})\\propto P(X_{1}|y)...P(X_{n}|y)P(y)$$\n",
    "where $X_{1}$ to $X_{n}$ are input features. You can caclulate the likelihood probability by multiplying all conditional probability of the features $X_{i}$ given label $y$ which can be expressed as:\n",
    "$$P(y|X_{1},...,X_{n})\\propto P(y)\\prod_{i=1}^{n}P(X_{i}|y)$$\n",
    "\n",
    "Back to our example, let's say today is cloudy, the temperature is cool, and the humidity is normal, can you predict will it rain today? Well, let's apply the same steps as in the example in part 1. Here, we want to calculate the following:\n",
    "\n",
    "$$P(y=yes|X_{1}=cloudy,X_{2}=cool,X_{3}=normal) \\propto P(y=yes)P(X_{1}=cloudy|y=yes)P(X_{2}=cool|y=yes)P(X_{3}=normal|y=yes)$$\n",
    "$$P(y=no|X_{1}=cloudy,X_{2}=cool,X_{3}=normal) \\propto P(y=no)P(X_{1}=cloudy|y=no)P(X_{2}=cool|y=no)P(X_{3}=normal|y=no)$$\n",
    "\n",
    "First, create a frequency table for each feature of the training data as follows:\n",
    "\n",
    "| Outlook | Yes   | No     | Total |\n",
    "|---------|-------|--------|-------|\n",
    "| Cloudy  | 5     | 3      | 8     |\n",
    "| Sunny   | 1     | 6      | 7     |\n",
    "| Total   | 6     | 9      | 15    |\n",
    "\n",
    "| Temperature | Yes   | No     | Total |\n",
    "|-------------|-------|--------|-------|\n",
    "| Cool        | 4     | 2      | 6     |\n",
    "| Mild        | 1     | 3      | 4     |\n",
    "| Hot         | 1     | 4      | 5     |\n",
    "| Total       | 6     | 9      | 15    |\n",
    "\n",
    "| Humidity | Yes   | No     | Total |\n",
    "|----------|-------|--------|-------|\n",
    "| Low      | 1     | 5      | 6     |\n",
    "| Normal   | 2     | 2      | 4     |\n",
    "| High     | 3     | 2      | 5     |\n",
    "| Total    | 6     | 9      | 15    |\n",
    "\n",
    "#### **Step 1: compute the probabilities for each value of the label**\n",
    "Out of 15 observations, you have 6 yes and 9 no. So the respective probabilities are:\n",
    "$$P(y=yes)=\\frac{6}{15}$$\n",
    "$$P(y=no)=\\frac{9}{15}$$\n",
    "\n",
    "#### **Step 2: compute the conditional probability**\n",
    "Out of 6 yes, you have 5 cloudy. So the probability is:\n",
    "$$P(X_{1}=cloudy|y=yes)=\\frac{5}{6}$$\n",
    "Out of 6 yes, you have 4 cool. So the probability is:\n",
    "$$P(X_{2}=cool|y=yes)=\\frac{4}{6}$$\n",
    "Out of 6 yes, you have 2 normal. So the probability is:\n",
    "$$P(X_{3}=normal|y=yes)=\\frac{2}{6}$$\n",
    "Out of 9 no, you have 3 cloudy. So the probability is:\n",
    "$$P(X_{1}=cloudy|y=no)=\\frac{3}{9}$$\n",
    "Out of 9 no, you have 2 cool. So the probability is:\n",
    "$$P(X_{2}=cool|y=no)=\\frac{2}{9}$$\n",
    "Out of 9 no, you have 2 normal. So the probability is:\n",
    "$$P(X_{3}=normal|y=no)=\\frac{2}{9}$$\n",
    "\n",
    "#### **Step 3: subtitute all the three probabilities into the Naive Bayes formula**\n",
    "$$P(y=yes|X_{1}=cloudy,X_{2}=cool,X_{3}=normal) \\propto \\frac{6}{15}\\frac{5}{6}\\frac{4}{6}\\frac{2}{6}=\\frac{240}{3240}=0.07407407407$$\n",
    "$$P(y=no|X_{1}=cloudy,X_{2}=cool,X_{3}=normal) \\propto \\frac{9}{15}\\frac{3}{9}\\frac{2}{9}\\frac{2}{9}=\\frac{108}{10935}=0.0098765432$$\n",
    "\n",
    "Since $P(y=yes|X_{1}=cloudy,X_{2}=cool,X_{3}=normal) > P(y=no|X_{1}=cloudy,X_{2}=cool,X_{3}=normal)$, which means 'yes' get higher probability than 'no', then 'yes' will be our predicted output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Naive Bayes Classifier Formula (Updated)\n",
    "To be more mathematically precise, you can rewrite the Naive Bayes classifier formula in this form:\n",
    "$$\\hat{y}=\\arg\\max_{y}P(y)\\prod_{i=1}^{n}P(X_{i}|y)$$\n",
    "where $\\hat{y}$ is the prediction. In statistics, the hat is used to denote an estimator or an estimated/predicted value. The $\\arg\\max$ of a function is the value of the input, i.e. the 'argument' at the maximum. In other words, it is the label $y$ that has maximum probability. In the above example, the label $y$ that has maximum probability is 'yes'. So, $\\hat{y}=yes$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
